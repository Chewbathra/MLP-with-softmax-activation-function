{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifiers and MLP with levelwise dertivatives\n",
    "\n",
    "**IMPORTANT**:\n",
    "This Notebook is based on Assignment 1 given during the course \"CS231n: \n",
    "Convolutional Neural Networks for Visual Recognition\" at Stanford University.\n",
    "\n",
    "In this TP you have to implement two classifiers; Softmax and MLP. The softmax is the same as you did in last TP, but here we ask you to give all intermediate derivatives. The MLP is simply an extended version of the softmax classifier with one more layyer.\n",
    "\n",
    "You have the skeleton code and only need to write a few lines of code. What is important in this TP is not your code but your understanding of the problem. That's why we ask you to write and derive all the formulas on your report before implementing them. We will be vigilant regarding the correspondence of formulas and code.\n",
    "\n",
    "\n",
    "Here is a summary of what you will have to do :\n",
    "- *find* the formulas you need to implement, by doing the backward pass manually.\n",
    "- *report* these formulas in your report\n",
    "- *implement* the formulas you derive\n",
    "- *test* if your code works\n",
    "\n",
    "**LOOPS ARE NOT ALLOWED**. You must be able to write all the code you are asked for without loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make figures appear inline\n",
    "%matplotlib inline\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax classifier\n",
    "\n",
    "We ask you to implement a softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the classes\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "print(\"Visualizing some samples\")\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# choising parameters for subsampling\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "\n",
    "# subsample the data\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# Normalize the data: subtract the mean image and divide by the std\n",
    "mean_image = np.mean(X_train, axis = 0)\n",
    "std_image = np.std(X_train, axis = 0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# add bias dimension and transform into columns\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "\n",
    "num_dims = X_train.shape[1]\n",
    "\n",
    "# Printing dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the forward pass! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the forward method.\n",
    "# Open the file softmax_classifier.py and implement the\n",
    "# forward method.\n",
    "from softmax_classifier import SoftmaxClassifier\n",
    "\n",
    "sum_probs_test = [3686.68917834, 7316.67343288, 2545.80815387, 5886.5694029,  6709.66593813,\n",
    "                  6151.76263585, 4745.42478883, 4683.19162857, 2478.14829358, 4796.06654704]\n",
    "\n",
    "model = SoftmaxClassifier(num_dims, num_classes, random_seed=13)\n",
    "probs = model.forward(X_train)\n",
    "                                  \n",
    "if probs is None:\n",
    "    print(\"You have to implement scores first.\")\n",
    "else:\n",
    "    if np.abs(probs.sum(0) - np.array(sum_probs_test)).sum() < 1e-7:\n",
    "        print(\"Great! Your implementation of scores seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of scores seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implement the loss method ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file softmax_classifier.py and implement the\n",
    "# loss method.\n",
    "from softmax_classifier import SoftmaxClassifier\n",
    "\n",
    "model = SoftmaxClassifier(num_dims, num_classes, random_seed=13)\n",
    "_ = model.forward(X_train)\n",
    "loss = model.loss(X_train, y_train, 0.0)\n",
    "\n",
    "if loss is None:\n",
    "    print(\"You have to implement loss first.\")\n",
    "else:\n",
    "    if np.abs(loss - 5.65453535) < 1e-7:\n",
    "        print(\"Great! Your implementation of the loss seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of  the loss seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the computation of the gradients in backward method! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file softmax_classifier.py and implement the\n",
    "# backward method.\n",
    "from softmax_classifier import SoftmaxClassifier\n",
    "\n",
    "N = X_train.shape[0]\n",
    "\n",
    "model = SoftmaxClassifier(num_dims, num_classes, random_seed=13)\n",
    "# before backward we need to forward\n",
    "Q = model.forward(X_train)\n",
    "grads = model.backward(X_train, y_train, 0.0)\n",
    "\n",
    "\n",
    "\n",
    "if grads is None:\n",
    "    print(\"You have to implement the gradients first.\")\n",
    "else:\n",
    "    # computing gradients with the batch version\n",
    "    C = np.zeros((N, num_classes))\n",
    "    C[range(N), y_train] = 1\n",
    "    grads_analytic = (X_train).T.dot(Q - C)\n",
    "    if grads_analytic.shape == grads.shape \\\n",
    "    and np.sum(grads_analytic) - np.sum(grads) < 1e-7 \\\n",
    "    and np.max(grads_analytic) - np.max(grads) < 1e-7 \\\n",
    "    and np.min(grads_analytic) - np.min(grads) < 1e-7:\n",
    "        print(\"Great! Your implementation of gradients seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of gradients seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are getting into throubles during the implementation of the gradients, try to use the cell below to debug your gradiens. \n",
    "\n",
    "First think about the dimensions the gradients should have the same shape has the element they are for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell check all local derivatives to try to help you find a bug.\n",
    "# If the previous code cell returns \"great\", this one should not give you any error.\n",
    "from softmax_classifier import SoftmaxClassifier\n",
    "\n",
    "def same_content(A, B):\n",
    "    if np.array_equal(A, np.zeros(1)):\n",
    "        return False\n",
    "    A = [A.sum(), A.max(), A.min()]\n",
    "    if (np.array(A) - np.array(B)).sum() < 1e-5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "dLoss_dProbs = [-3810211946183.7754, -0.0, -3147476732219.055]\n",
    "dProbs_dScores = [5.2149257134814775e-14, 0.24999999999736983, -0.24990727447134475]\n",
    "dLoss_dScores = [1.141309269314661e-13, 0.9999981661360309, -0.9999999999996821]\n",
    "dScores_dW = [49000.00000002424, 155.031, -140.29748979591838]\n",
    "dLoss_dW = [0.26850115388515405, 579293.6998373481, -463509.5137372456]\n",
    "dScores_dX = [0.1342505763011005, 0.004442371066049412, -0.004706426140736417]\n",
    "dLoss_dX = [112.28116582955269, 0.006450477708706644, -0.006450002972772494]\n",
    "\n",
    "\n",
    "\n",
    "N = X_train.shape[0]\n",
    "model = SoftmaxClassifier(num_dims, num_classes, random_seed=13)\n",
    "# before backward we need to forward\n",
    "_ = model.forward(X_train)\n",
    "_ = model.backward(X_train, y_train, 1.0, dP_dS=True)\n",
    "\n",
    "content = \"\"\n",
    "\n",
    "# LEVEL 1\n",
    "# dLoss / dProbs\n",
    "content += \"\\ndLoss / dProbs :\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dProbs.shape == (N, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dProbs, dLoss_dProbs) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# LEVEL 2\n",
    "# dProbs / dScores\n",
    "content += \"\\ndProbs / dScores:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dProbs_dScores.shape == (N, num_classes, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dProbs_dScores, dProbs_dScores) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dScores\n",
    "content += \"\\nLoss / dScores:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dScores.shape == (N, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dScores, dLoss_dScores) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# LEVEL 3\n",
    "# dScores / dWeights\n",
    "content += \"\\ndScores / dWeights:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dScores_dW.shape == (N, num_dims) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dScores_dW, dScores_dW) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dWeights\n",
    "content += \"\\ndLoss / dWeights:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dW.shape == (num_dims, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dW, dLoss_dW) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dScores / dX\n",
    "content += \"\\ndScores / dX:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dScores_dX.shape == (num_dims, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dScores_dX, dScores_dX) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dX\n",
    "content += \"\\ndLoss / dX:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dX.shape == (N, num_dims) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dX, dLoss_dX) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start playing, we need to implement the prediction method of the classifier. Implement it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file softmax_classifier.py and implement the\n",
    "# predict method.\n",
    "from softmax_classifier import SoftmaxClassifier\n",
    "\n",
    "model = SoftmaxClassifier(num_dims, num_classes, random_seed=13)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "if not np.sum(y_pred):\n",
    "    print(\"You have to implement the gradients first.\")\n",
    "else:\n",
    "    if np.abs(np.sum(y_pred) - 210313.) < 1e-7:\n",
    "        print(\"Great! Your implementation of gradients seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of gradients seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use validation to tune the hyperparameters. To reduce the computational time at this step we will reduce the dataset. Performing the computation of all the derivatives takes some time.\n",
    "\n",
    "Don't forget to vectorize your formulas, loops are very expensive !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_idxs(samples_per_class, y_data):\n",
    "    idx_keep = []\n",
    "    for y, cls in enumerate(classes):\n",
    "        idxs = np.flatnonzero(y_data == y)\n",
    "        idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "        idx_keep += idxs.tolist()\n",
    "        np.random.shuffle(idx_keep) \n",
    "    return idx_keep\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "samples_per_class_train = 500\n",
    "samples_per_class_val = 50\n",
    "samples_per_class_test = 100\n",
    "\n",
    "idx_keep_train = get_samples_idxs(samples_per_class_train, y_train)\n",
    "idx_keep_val = get_samples_idxs(samples_per_class_val, y_val)\n",
    "idx_keep_test = get_samples_idxs(samples_per_class_test, y_test)\n",
    "\n",
    "\n",
    "# subsample the data\n",
    "X_train = X_train[idx_keep_train]\n",
    "y_train = y_train[idx_keep_train]\n",
    "\n",
    "X_val = X_val[idx_keep_val]\n",
    "y_val = y_val[idx_keep_val]\n",
    "\n",
    "X_test = X_test[idx_keep_test]\n",
    "y_test = y_test[idx_keep_test]\n",
    "\n",
    "\n",
    "# Printing dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from softmax_classifier import SoftmaxClassifier\n",
    "import copy\n",
    "\n",
    "# to save loss of best model\n",
    "best_hist = {}\n",
    "# to save accuracy on validation set of best model\n",
    "best_val = -1\n",
    "# to save best model\n",
    "best_model = None\n",
    "best_lr = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "learning_rates = [1e-7]\n",
    "regularization_strengths = [1e2, 1e4]\n",
    "# number of iterations to train\n",
    "num_iters = 1500\n",
    "# if true display informations about training\n",
    "verbose = True\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        print(\"lr = {}, reg = {}\".format(lr, reg))\n",
    "        model = SoftmaxClassifier(num_dims, num_classes, random_seed=13)\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "        # set. For each combination of hyperparameters, train a model on the           #\n",
    "        # training set, compute its accuracy on the training and validation sets, and  #\n",
    "        # store the best validation accuracy in best_val and the model object that     #\n",
    "        # achieves this accuracy in best_logistc.                                      #\n",
    "        #                                                                              #\n",
    "        # Hint: You should use a small value for num_iters as you develop your         #\n",
    "        # validation code so that the model don't take much time to train; once you are#\n",
    "        # confident that your validation code works, you should rerun the validation   #\n",
    "        # code with a larger value for num_iters, lets say 1500.                       #\n",
    "        #                                                                              #\n",
    "        # To copy the model use best_model = copy.deepcopy(model)                      #\n",
    "        ################################################################################\n",
    "        history= model.train(X_train, y_train, num_iters, lr, reg, verbose=True)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        acc_train = np.mean(y_train_pred == y_train)\n",
    "        \n",
    "        y_val_pred = model.predict(X_val)\n",
    "        acc_val = np.mean(y_val_pred == y_val)\n",
    "        \n",
    "        if(acc_val > best_val):\n",
    "            best_val = acc_val\n",
    "            best_hist = history\n",
    "            best_model = copy.deepcopy(model)\n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "        print(\"\\r\\t -> train acc = {:.3f}, val acc = {:.3f}\".format(acc_train, acc_val))\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: {:.3f}'.format(best_val))\n",
    "plt.plot(best_hist[\"train_loss\"], label=\"train loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the best model, we can test the accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Logistic on raw pixels final test set accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally visualizy the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_model.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "We ask you now to extend your softmax classifier with one more layer. The resulting network will be a multilayer perceptron (MLP).\n",
    "\n",
    "Most of the code you did in the softmax classifier can be reused. The only methods where you need to add some lines of code are forward and backward.\n",
    "\n",
    "In forward you need to take into account the new layer duing the computation of the probabilities.\n",
    "In backward you need to compute the new gradients with respect to the new set of weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make figures appear inline\n",
    "%matplotlib inline\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the classes\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "print(\"Visualizing some samples\")\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# choising parameters for subsampling\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "\n",
    "# subsample the data\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# Normalize the data: subtract the mean image and divide by the std\n",
    "mean_image = np.mean(X_train, axis = 0)\n",
    "std_image = np.std(X_train, axis = 0)\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "\n",
    "# add bias dimension and transform into columns\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "\n",
    "num_dims = X_train.shape[1]\n",
    "\n",
    "# Printing dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the forward pass! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the forward method.\n",
    "# Open the file softmax_classifier.py and implement the\n",
    "# forward method.\n",
    "from mlp import MLP\n",
    "\n",
    "sum_probs_test = [4898.757432689954, 4902.225015570076, 4899.382555984644, 4900.70491757786, \n",
    "                  4898.454662888324, 4899.021295482354, 4898.850139082313, 4901.748730855941,\n",
    "                  4900.350478627343, 4900.504771241191]\n",
    "\n",
    "model = MLP(num_dims, 128, num_classes, random_seed=13)\n",
    "probs = model.forward(X_train)\n",
    "\n",
    "if probs is None:\n",
    "    print(\"You have to implement scores first.\")\n",
    "else:\n",
    "    if np.abs(probs.sum(0) - np.array(sum_probs_test)).sum() < 1e-7:\n",
    "        print(\"Great! Your implementation of scores seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of scores seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then implement the loss method ! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file softmax_classifier.py and implement the\n",
    "# loss method.\n",
    "from mlp import MLP\n",
    "\n",
    "model = MLP(num_dims, 128, num_classes, random_seed=13)\n",
    "_ = model.forward(X_train)\n",
    "loss = model.loss(X_train, y_train, 0.0)\n",
    "\n",
    "if loss is None:\n",
    "    print(\"You have to implement loss first.\")\n",
    "else:\n",
    "    if np.abs(loss - 2.303255307) < 1e-7:\n",
    "        print(\"Great! Your implementation of the loss seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of  the loss seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the computation of the gradients in backward method! Be sure to put the formulas on the report first !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file softmax_classifier.py and implement the\n",
    "# backward method.\n",
    "from mlp import MLP\n",
    "\n",
    "\n",
    "N = X_train.shape[0]\n",
    "\n",
    "model = MLP(num_dims, 128, num_classes, random_seed=13)\n",
    "# before backward we need to forward\n",
    "Q = model.forward(X_train)\n",
    "grads = model.backward(X_train, y_train, 0.0)\n",
    "\n",
    "\n",
    "\n",
    "if grads[\"W1\"] is None or grads[\"W2\"] is None:\n",
    "    print(\"You have to implement the gradients first.\")\n",
    "else:\n",
    "    # computing gradients with the batch version\n",
    "    C = np.zeros((N, num_classes))\n",
    "    C[range(N), y_train] = 1\n",
    "\n",
    "    grads_analytic_W1 = X_train.T.dot((Q - C).dot(model.W2.T))\n",
    "    grads_analytic_W2 = (X_train.dot(model.W1)).T.dot(Q - C)\n",
    "    if np.isclose(grads_analytic_W1, grads[\"W1\"]).all()\\\n",
    "    and np.isclose(grads_analytic_W2, grads[\"W2\"]).all():\n",
    "        print(\"Great! Your implementation of gradients seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of gradients seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are getting into throubles during the implementation of the gradients, try to use the cell below to debug your gradiens. \n",
    "\n",
    "First think about the dimensions the gradients should have the same shape has the element they are for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell check all local derivatives to try to help you find a bug.\n",
    "# If the previous code cell returns \"great\", this one should not give you any error.\n",
    "from mlp import MLP\n",
    "\n",
    "def same_content(A, B):\n",
    "    if np.array_equal(A, np.zeros(1)):\n",
    "        return False\n",
    "    A = [A.sum(), A.max(), A.min()]\n",
    "    if (np.array(A) - np.array(B)).sum() < 1e-5:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "dLoss_dProbs = [-490656.7017266333, -0.0, -12.012981542323267]\n",
    "dProbs_dScores = [1.0283440765590512e-14,0.10610655035714026, -0.013556167401344707]\n",
    "dLoss_dScores = [1.7486012637846216e-14, 0.12066709918218307, -0.9167567188481167]\n",
    "dScores_dW2 = [-141.41495734416333, 22.357396893080075, -25.91429233948654]\n",
    "dLoss_dW2 = [0.01522059488343075, 15804.418859398394, -19162.711858338203]\n",
    "dScores_dHidden = [0.007610297393284419, 0.0029887207179070427, -0.0033672909837626965]\n",
    "dLoss_dHidden = [0.27043683338251784, 0.0032325229740335776, -0.0028077294850832007]\n",
    "dHidden_dW1 = [49000.00000002424, 155.031, -140.29748979591838]\n",
    "dLoss_dW1 = [3321671.243764238, 1362.732437946318, -1675.285768006116]\n",
    "dHidden_dX = [-0.4542162563738851, 0.004633008549146779, -0.004709407748146688]\n",
    "dLoss_dX = [-0.0014711047361969005, 4.3748240697223114e-05, -4.510151419439094e-05]\n",
    "\n",
    "\n",
    "\n",
    "N = X_train.shape[0]\n",
    "num_hidden = 128\n",
    "model = MLP(num_dims, num_hidden, num_classes, random_seed=13)\n",
    "# before backward we need to forward\n",
    "_ = model.forward(X_train)\n",
    "_ = model.backward(X_train, y_train, 1.0, dP_dS=True)\n",
    "\n",
    "content = \"\"\n",
    "\n",
    "\n",
    "# LEVEL 1\n",
    "# dLoss / dProbs\n",
    "content += \"\\ndLoss / dProbs :\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dProbs.shape == (N, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dProbs, dLoss_dProbs) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# LEVEL 2\n",
    "# dProbs / dScores\n",
    "content += \"\\ndProbs / dScores:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dProbs_dScores.shape == (N, num_classes, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dProbs_dScores, dProbs_dScores) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dScores\n",
    "content += \"\\nLoss / dScores:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dScores.shape == (N, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dScores, dLoss_dScores) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# LEVEL 3\n",
    "# dScores / dW2\n",
    "content += \"\\ndScores / dW2:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dScores_dW2.shape == (N, num_hidden) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dScores_dW2, dScores_dW2) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dW2\n",
    "content += \"\\ndLoss / dW2:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dW2.shape == (num_hidden, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dW2, dLoss_dW2) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dScores / dHidden\n",
    "content += \"\\ndScores / dHidden:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dScores_dHidden.shape == (num_hidden, num_classes) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dScores_dHidden, dScores_dHidden) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dHidden\n",
    "content += \"\\ndLoss / dHidden:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dHidden.shape == (N, num_hidden) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dHidden, dLoss_dHidden) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# LEVEL 4\n",
    "# dHidden / dW1\n",
    "content += \"\\ndHidden / dW1:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dHidden_dW1.shape == (N, num_dims) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dHidden_dW1, dHidden_dW1) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dW1\n",
    "content += \"\\ndLoss / dW1:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dW1.shape == (num_dims, num_hidden) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dW1, dLoss_dW1) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dHidden / dX\n",
    "content += \"\\ndScores / dX:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dHidden_dX.shape == (num_dims, num_hidden) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dHidden_dX, dHidden_dX) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "# dLoss / dX\n",
    "content += \"\\ndLoss / dX:\\n\"\n",
    "content += \"\\t Shape: OK!\\n\" if model.dLoss_dX.shape == (N, num_dims) else \"\\t Shape: NOT OK!\\n\"\n",
    "content += \"\\t Content: OK!\\n\"if same_content(model.dLoss_dX, dLoss_dHidden) else \"\\t Content: NOT OK!\\n\"\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start playing, we need to implement the prediction method of the classifier. Implement it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file softmax_classifier.py and implement the\n",
    "# predict method.\n",
    "from mlp import MLP\n",
    "\n",
    "model = MLP(num_dims, 128, num_classes, random_seed=13)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "if not np.sum(y_pred):\n",
    "    print(\"You have to implement the gradients first.\")\n",
    "else:\n",
    "    if np.abs(np.sum(y_pred) - 227889.) < 1e-7:\n",
    "        print(\"Great! Your implementation of gradients seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of gradients seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use validation to tune the hyperparameters. To reduce the computational time at this step we will reduce the dataset. Performing the computation of all the derivatives takes some time.\n",
    "\n",
    "Don't forget to vectorize your formulas, loops are very expensive !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_idxs(samples_per_class, y_data):\n",
    "    idx_keep = []\n",
    "    for y, cls in enumerate(classes):\n",
    "        idxs = np.flatnonzero(y_data == y)\n",
    "        idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "        idx_keep += idxs.tolist()\n",
    "        np.random.shuffle(idx_keep) \n",
    "    return idx_keep\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "samples_per_class_train = 200\n",
    "samples_per_class_val = 25\n",
    "samples_per_class_test = 50\n",
    "\n",
    "idx_keep_train = get_samples_idxs(samples_per_class_train, y_train)\n",
    "idx_keep_val = get_samples_idxs(samples_per_class_val, y_val)\n",
    "idx_keep_test = get_samples_idxs(samples_per_class_test, y_test)\n",
    "\n",
    "\n",
    "# subsample the data\n",
    "X_train = X_train[idx_keep_train]\n",
    "y_train = y_train[idx_keep_train]\n",
    "\n",
    "X_val = X_val[idx_keep_val]\n",
    "y_val = y_val[idx_keep_val]\n",
    "\n",
    "X_test = X_test[idx_keep_test]\n",
    "y_test = y_test[idx_keep_test]\n",
    "\n",
    "\n",
    "# Printing dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from mlp import MLP\n",
    "import copy\n",
    "\n",
    "# to save loss of best model\n",
    "best_hist = {}\n",
    "# to save accuracy on validation set of best model\n",
    "best_val = -1\n",
    "# to save best model\n",
    "best_model = None\n",
    "best_lr = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "learning_rates = [1e-5]\n",
    "regularization_strengths = [1e-2, 0, 1e2]\n",
    "# number of iterations to train\n",
    "num_iters = 1500\n",
    "# if true display informations about training\n",
    "verbose = True\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        print(\"lr = {}, reg = {}\".format(lr, reg))\n",
    "        model = MLP(num_dims, 128, num_classes, random_seed=13)\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "        # set. For each combination of hyperparameters, train a model on the           #\n",
    "        # training set, compute its accuracy on the training and validation sets, and  #\n",
    "        # store the best validation accuracy in best_val and the model object that     #\n",
    "        # achieves this accuracy in best_logistc.                                      #\n",
    "        #                                                                              #\n",
    "        # Hint: You should use a small value for num_iters as you develop your         #\n",
    "        # validation code so that the model don't take much time to train; once you are#\n",
    "        # confident that your validation code works, you should rerun the validation   #\n",
    "        # code with a larger value for num_iters, lets say 2000.                       #\n",
    "        #                                                                              #\n",
    "        # To copy the model use best_model = copy.deepcopy(model)                      #\n",
    "        ################################################################################\n",
    "        history= model.train(X_train, y_train, num_iters, lr, reg, verbose=True)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        acc_train = np.mean(y_train_pred == y_train)\n",
    "        \n",
    "        y_val_pred = model.predict(X_val)\n",
    "        acc_val = np.mean(y_val_pred == y_val)\n",
    "        \n",
    "        if(acc_val > best_val):\n",
    "            best_val = acc_val\n",
    "            best_hist = history\n",
    "            best_model = copy.deepcopy(model)\n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "        print(\"\\r\\t -> train acc = {:.3f}, val acc = {:.3f}\".format(acc_train, acc_val))\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved during cross-validation: {:.3f}'.format(best_val))\n",
    "plt.plot(best_hist[\"train_loss\"], label=\"train loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the best model, we can test the accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('Logistic on raw pixels final test set accuracy: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally visualizy the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_model.W1[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 128)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
